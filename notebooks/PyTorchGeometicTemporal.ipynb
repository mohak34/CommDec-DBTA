{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8a23c6",
   "metadata": {},
   "source": [
    "# PyTorch Geometric Temporal for Dynamic Network Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c71c67",
   "metadata": {},
   "source": [
    "This notebook explores the implementation of PyTorch Geometric Temporal, focusing on its application in the research project: *Implementing Decay-Based Temporal Attention for Dynamic Network Adaptation*.\n",
    "\n",
    "Implementation based on: https://github.com/StatsDLMathsRecomSys/Inductive-representation-learning-on-temporal-graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c0c59c",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of required libraries\n",
    "# Ensure you have PyTorch installed. If not, uncomment and run the appropriate command from https://pytorch.org/get-started/locally/ and for CUDA enabled GPU support use cuda12.8\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "\n",
    "# Install PyTorch Geometric and PyTorch Geometric Temporal\n",
    "!pip install torch-geometric\n",
    "!pip install torch-geometric-temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6aa4ac",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f06eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu128\n",
      "PyTorch Geometric version: 2.6.1\n",
      "PyTorch Geometric Temporal version: 0.54.0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch_geometric\n",
    "# import torch_geometric_temporal\n",
    "\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "# print(f\"PyTorch Geometric Temporal version: {torch_geometric_temporal.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7098aba",
   "metadata": {},
   "source": [
    "## 3. Baselining the TGAT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d85687",
   "metadata": {},
   "source": [
    "This section will focus on building and utilizing models related to TGAT implementation, particularly those that can be adapted or extended for decay-based temporal attention mechanisms in dynamic networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c4836",
   "metadata": {},
   "source": [
    "### 3.1. Baseline Implementation: TGAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85865a59",
   "metadata": {},
   "source": [
    "We'll start by implementing the Temporal Graph Attention Network (TGAT) model from the [Inductive representation learning on temporal graphs paper](https://arxiv.org/abs/2002.07962). This implementation follows the original source code available on [GitHub](https://github.com/StatsDLMathsRecomSys/Inductive-representation-learning-on-temporal-graphs). We will then extend this model to incorporate a decay mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d88b0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize model\\nmodel = TGAT(\\n    node_features_dim=100,  # Dimension of node features\\n    edge_features_dim=50,    # Dimension of edge features \\n    time_dim=10,            # Dimension of time encoding\\n    hidden_dim=128,         # Hidden dimension size\\n    num_layers=2,           # Number of TGAT layers\\n    num_heads=2,            # Number of attention heads\\n    dropout=0.1            # Dropout rate\\n)\\n\\n# Example data\\nbatch_size = 32\\nnum_neighbors = 20\\n\\n# Node features\\nsrc_features = torch.randn(batch_size, 100)\\ndst_features = torch.randn(batch_size, 100)\\n\\n# Neighbor indices\\nsrc_neighbor_indices = torch.randint(0, 1000, (batch_size, num_neighbors))\\ndst_neighbor_indices = torch.randint(0, 1000, (batch_size, num_neighbors))\\n\\n# Neighbor timestamps\\nsrc_neighbor_times = torch.rand(batch_size, num_neighbors)\\ndst_neighbor_times = torch.rand(batch_size, num_neighbors)\\n\\n# Edge features\\nsrc_edge_features = torch.randn(batch_size, num_neighbors, 50)\\ndst_edge_features = torch.randn(batch_size, num_neighbors, 50)\\n\\n# Forward pass\\nlink_prob = model(src_features, dst_features, src_neighbor_indices, dst_neighbor_indices,\\n                 src_neighbor_times, dst_neighbor_times, edge_features_src=src_edge_features,\\n                 edge_features_dst=dst_edge_features)\\n\\nprint(link_prob.shape)  # Should be [batch_size, 1]\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Time Encoding Module\n",
    "class TimeEncoder(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(TimeEncoder, self).__init__()\n",
    "        self.dimension = dimension\n",
    "        self.w = nn.Linear(1, dimension)\n",
    "        \n",
    "        # Initialize with non-trainable fixed frequencies\n",
    "        self.w.weight = nn.Parameter((torch.from_numpy(1 / 10 ** np.linspace(0, 9, dimension))).float().reshape(dimension, 1))\n",
    "        self.w.bias = nn.Parameter(torch.zeros(dimension))\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # t has shape [batch_size, 1] or [batch_size]\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(1)  # [batch_size] -> [batch_size, 1]\n",
    "        \n",
    "        # Return shape [batch_size, dimension]\n",
    "        return torch.cos(self.w(t))\n",
    "\n",
    "# Multi-Head Attention Layer\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        \n",
    "        # Linear projections for Query, Key, Value\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        \n",
    "        # Final output projection\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        batch_size, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n",
    "        \n",
    "        residual = q  # For residual connection\n",
    "        \n",
    "        # Linear projections and reshape\n",
    "        q = self.w_qs(q).view(batch_size, len_q, n_head, d_k)  # [b, lq, n, dk]\n",
    "        k = self.w_ks(k).view(batch_size, len_k, n_head, d_k)  # [b, lk, n, dk]\n",
    "        v = self.w_vs(v).view(batch_size, len_v, n_head, d_v)  # [b, lv, n, dv]\n",
    "        \n",
    "        # Transpose for attention calculation\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)  # [b, n, lq, dk], [b, n, lk, dk], [b, n, lv, dv]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(d_k)  # [b, n, lq, lk]\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)  # Apply mask (if provided)\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)  # [b, n, lq, lk]\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # Multiply attention with values\n",
    "        output = torch.matmul(attn, v)  # [b, n, lq, dv]\n",
    "        \n",
    "        # Reshape back and apply output projection\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, len_q, -1)  # [b, lq, n*dv]\n",
    "        output = self.fc(output)  # [b, lq, d_model]\n",
    "        \n",
    "        # Apply residual connection and layer normalization\n",
    "        output = self.layer_norm(output + residual)\n",
    "        \n",
    "        return output, attn\n",
    "\n",
    "# Feed-Forward Network\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(residual + x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Temporal Graph Attention Layer\n",
    "class TGATLayer(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, d_time, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout)\n",
    "        self.time_enc = TimeEncoder(d_time)\n",
    "        self.position_ffn = PositionwiseFeedForward(d_model, d_model * 4, dropout)\n",
    "        \n",
    "    def forward(self, node_features, neighbor_features, edge_times, masks=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            node_features: [batch_size, d_model]\n",
    "            neighbor_features: [batch_size, num_neighbors, d_model]\n",
    "            edge_times: [batch_size, num_neighbors]\n",
    "            masks: [batch_size, num_neighbors] (optional)\n",
    "        Returns:\n",
    "            output: [batch_size, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, num_neighbors = neighbor_features.size(0), neighbor_features.size(1)\n",
    "        \n",
    "        # Encode the time information\n",
    "        time_features = self.time_enc(edge_times.view(-1, 1)).view(batch_size, num_neighbors, -1)  # [b, num_n, d_time]\n",
    "        \n",
    "        # Add time encoding to neighbor features\n",
    "        time_enhanced_features = torch.cat([neighbor_features, time_features], dim=-1)\n",
    "        \n",
    "        # Expand node features for attention\n",
    "        node_features_expanded = node_features.unsqueeze(1)  # [b, 1, d_model]\n",
    "        \n",
    "        # Apply attention between the node and its neighbors\n",
    "        attn_output, attn_weights = self.attention(node_features_expanded, time_enhanced_features, time_enhanced_features, masks)\n",
    "        \n",
    "        # Apply feed-forward network\n",
    "        output = self.position_ffn(attn_output.squeeze(1))  # [b, d_model]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Complete TGAT Model\n",
    "class TGAT(nn.Module):\n",
    "    def __init__(self, node_features_dim, edge_features_dim, time_dim, hidden_dim, num_layers, num_heads, dropout=0.1):\n",
    "        super(TGAT, self).__init__()\n",
    "        \n",
    "        self.node_features_dim = node_features_dim\n",
    "        self.edge_features_dim = edge_features_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Initial node embedding layer\n",
    "        self.node_embedding = nn.Linear(node_features_dim, hidden_dim)\n",
    "        \n",
    "        # Edge embedding layer\n",
    "        self.edge_embedding = nn.Linear(edge_features_dim, hidden_dim)\n",
    "        \n",
    "        # TGAT layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TGATLayer(num_heads, hidden_dim + edge_features_dim, \n",
    "                      hidden_dim // num_heads, hidden_dim // num_heads, \n",
    "                      time_dim, dropout) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # For link prediction\n",
    "        self.link_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def compute_temporal_embeddings(self, node_features, neighbor_indices, neighbor_times, neighbor_features=None, edge_features=None):\n",
    "        \"\"\"\n",
    "        Compute node embeddings using the temporal graph attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            node_features: Node features [batch_size, node_features_dim]\n",
    "            neighbor_indices: Indices of neighbors [batch_size, num_neighbors]\n",
    "            neighbor_times: Timestamps of neighbor connections [batch_size, num_neighbors]\n",
    "            neighbor_features: Features of neighbors [batch_size, num_neighbors, node_features_dim]\n",
    "            edge_features: Edge features [batch_size, num_neighbors, edge_features_dim]\n",
    "            \n",
    "        Returns:\n",
    "            node_embeddings: Computed temporal node embeddings [batch_size, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Initial embedding\n",
    "        node_embeddings = self.node_embedding(node_features)  # [b, hidden_dim]\n",
    "        \n",
    "        # If neighbor features are not provided, use a lookup table or zeros\n",
    "        if neighbor_features is None:\n",
    "            neighbor_embeddings = torch.zeros(node_features.size(0), neighbor_indices.size(1), self.hidden_dim, device=node_features.device)\n",
    "        else:\n",
    "            neighbor_embeddings = self.node_embedding(neighbor_features)  # [b, num_n, hidden_dim]\n",
    "        \n",
    "        # Process edge features if provided\n",
    "        if edge_features is not None:\n",
    "            edge_embeddings = self.edge_embedding(edge_features)  # [b, num_n, hidden_dim]\n",
    "            # Incorporate edge features into neighbor embeddings\n",
    "            neighbor_embeddings = torch.cat([neighbor_embeddings, edge_embeddings], dim=-1)  # [b, num_n, 2*hidden_dim]\n",
    "        \n",
    "        # Create masks if needed (e.g., for padding)\n",
    "        masks = None  # Set to appropriate mask if needed\n",
    "        \n",
    "        # Apply TGAT layers\n",
    "        x = node_embeddings\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, neighbor_embeddings, neighbor_times, masks)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src_features, dst_features, src_neighbor_indices, dst_neighbor_indices, \n",
    "                src_neighbor_times, dst_neighbor_times, src_neighbor_features=None, \n",
    "                dst_neighbor_features=None, src_edge_features=None, dst_edge_features=None):\n",
    "        \"\"\"\n",
    "        Forward pass for link prediction task.\n",
    "        \n",
    "        Returns:\n",
    "            link_prob: Probability of link existence [batch_size, 1]\n",
    "        \"\"\"\n",
    "        # Compute node embeddings for both source and destination nodes\n",
    "        src_embeddings = self.compute_temporal_embeddings(\n",
    "            src_features, src_neighbor_indices, src_neighbor_times, \n",
    "            src_neighbor_features, src_edge_features\n",
    "        )\n",
    "        \n",
    "        dst_embeddings = self.compute_temporal_embeddings(\n",
    "            dst_features, dst_neighbor_indices, dst_neighbor_times, \n",
    "            dst_neighbor_features, dst_edge_features\n",
    "        )\n",
    "        \n",
    "        # Concatenate embeddings for link prediction\n",
    "        link_features = torch.cat([src_embeddings, dst_embeddings], dim=1)  # [b, 2*hidden_dim]\n",
    "        \n",
    "        # Predict link\n",
    "        link_prob = self.link_predictor(link_features)\n",
    "        \n",
    "        return link_prob\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize model\n",
    "model = TGAT(\n",
    "    node_features_dim=100,  # Dimension of node features\n",
    "    edge_features_dim=50,    # Dimension of edge features \n",
    "    time_dim=10,            # Dimension of time encoding\n",
    "    hidden_dim=128,         # Hidden dimension size\n",
    "    num_layers=2,           # Number of TGAT layers\n",
    "    num_heads=2,            # Number of attention heads\n",
    "    dropout=0.1            # Dropout rate\n",
    ")\n",
    "\n",
    "# Example data\n",
    "batch_size = 32\n",
    "num_neighbors = 20\n",
    "\n",
    "# Node features\n",
    "src_features = torch.randn(batch_size, 100)\n",
    "dst_features = torch.randn(batch_size, 100)\n",
    "\n",
    "# Neighbor indices\n",
    "src_neighbor_indices = torch.randint(0, 1000, (batch_size, num_neighbors))\n",
    "dst_neighbor_indices = torch.randint(0, 1000, (batch_size, num_neighbors))\n",
    "\n",
    "# Neighbor timestamps\n",
    "src_neighbor_times = torch.rand(batch_size, num_neighbors)\n",
    "dst_neighbor_times = torch.rand(batch_size, num_neighbors)\n",
    "\n",
    "# Edge features\n",
    "src_edge_features = torch.randn(batch_size, num_neighbors, 50)\n",
    "dst_edge_features = torch.randn(batch_size, num_neighbors, 50)\n",
    "\n",
    "# Forward pass\n",
    "link_prob = model(src_features, dst_features, src_neighbor_indices, dst_neighbor_indices,\n",
    "                 src_neighbor_times, dst_neighbor_times, edge_features_src=src_edge_features,\n",
    "                 edge_features_dst=dst_edge_features)\n",
    "\n",
    "print(link_prob.shape)  # Should be [batch_size, 1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de789eec",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "\n",
    "This TGAT implementation consists of several components:\n",
    "\n",
    "1. **TimeEncoder**: Encodes time differences into feature vectors using a fixed set of frequencies. This allows the model to capture temporal patterns at different scales.\n",
    "\n",
    "2. **MultiHeadAttention**: Implements the standard multi-head attention mechanism from the Transformer architecture, enabling the model to attend to different parts of the neighborhood.\n",
    "\n",
    "3. **PositionwiseFeedForward**: A point-wise feed-forward network applied after the attention mechanism.\n",
    "\n",
    "4. **TGATLayer**: Combines the attention mechanism with time encoding to create a temporal graph attention layer that can process both structural and temporal information.\n",
    "\n",
    "5. **TGAT**: The full model that stacks multiple TGAT layers and includes components for link prediction.\n",
    "\n",
    "The key innovation in TGAT is the incorporation of temporal information directly into the attention mechanism, allowing the model to learn how node relationships evolve over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ab20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGAT model initialized with 2 layers and 4 attention heads.\n"
     ]
    }
   ],
   "source": [
    "# Example instantiation of the TGAT model\n",
    "\n",
    "# Node/edge feature dimensions - these should match your data\n",
    "node_features_dim = 64\n",
    "edge_features_dim = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "time_dim = 10\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "\n",
    "# Create the model\n",
    "tgat_model = TGAT(\n",
    "    node_features_dim=node_features_dim,\n",
    "    edge_features_dim=edge_features_dim,\n",
    "    time_dim=time_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "print(f\"TGAT model initialized with {num_layers} layers and {num_heads} attention heads.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30241e",
   "metadata": {},
   "source": [
    "### 3.2. Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da01743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3656745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c7b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598522c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commdec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
