{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c00c148",
   "metadata": {},
   "source": [
    "# Enhanced TGN Model with Visualizations\n",
    "\n",
    "This notebook demonstrates the Enhanced Temporal Graph Network (TGN) model with:\n",
    "1. Disabled decay factor in DecayTemporalAttention for evaluation without decay metrics\n",
    "2. Visualizations of the Reddit dataset in graph format\n",
    "\n",
    "We'll use the implementation from the main Enhanced_TGN.ipynb notebook but with these modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.enhanced_tgn import TemporalGraphNetwork\n",
    "from src.graph_visualization import visualize_temporal_graph, visualize_community_structure, visualize_temporal_communities\n",
    "from src.dataset_visualization import visualize_reddit_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfd262",
   "metadata": {},
   "source": [
    "## 1. Decay Factor in TGN\n",
    "\n",
    "We've modified the DecayTemporalAttention class to disable the decay factor by setting it to zero. Let's implement it here to show the change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayTemporalAttention(nn.Module):\n",
    "    \"\"\"Decay-based temporal attention mechanism for dynamic graphs.\n",
    "    This implements time-aware attention where attention weights decay over time.\n",
    "    We've disabled the decay effect by setting decay_factor to 0 for evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_feat_dim, edge_feat_dim, time_feat_dim, memory_dim, output_dim, \n",
    "                 n_heads=2, dropout=0.1, decay_factor=0.1):\n",
    "        super(DecayTemporalAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        # self.decay_factor = decay_factor\n",
    "        self.decay_factor = 0.0  # Setting to zero to disable decay effect\n",
    "        \n",
    "        # Dimension calculations\n",
    "        self.query_dim = node_feat_dim + time_feat_dim\n",
    "        self.key_dim = node_feat_dim + edge_feat_dim + time_feat_dim\n",
    "        self.value_dim = self.key_dim\n",
    "        \n",
    "        # Query, key, value projections\n",
    "        self.w_query = nn.Linear(self.query_dim, n_heads * memory_dim)\n",
    "        self.w_key = nn.Linear(self.key_dim, n_heads * memory_dim)\n",
    "        self.w_value = nn.Linear(self.value_dim, n_heads * memory_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_layer = nn.Linear(n_heads * memory_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "    \n",
    "    def forward(self, node_features, node_time_features, neighbor_features, \n",
    "                neighbor_time_features, edge_features, time_diffs, attention_mask=None):\n",
    "        \"\"\"Forward pass with decay-based temporal attention\"\"\"\n",
    "        batch_size, n_neighbors = neighbor_features.size(0), neighbor_features.size(1)\n",
    "        memory_dim = self.w_query.out_features // self.n_heads\n",
    "        \n",
    "        # Create query from node features and time features\n",
    "        query = torch.cat([node_features, node_time_features], dim=1)\n",
    "        query = self.w_query(query).view(batch_size, self.n_heads, memory_dim)\n",
    "        \n",
    "        # Create key/value from neighbor features\n",
    "        key_input = torch.cat([\n",
    "            neighbor_features.reshape(batch_size * n_neighbors, -1),\n",
    "            neighbor_time_features.reshape(batch_size * n_neighbors, -1),\n",
    "            edge_features.reshape(batch_size * n_neighbors, -1)\n",
    "        ], dim=1).view(batch_size, n_neighbors, -1)\n",
    "        \n",
    "        key = self.w_key(key_input).view(batch_size, n_neighbors, self.n_heads, memory_dim)\n",
    "        key = key.permute(0, 2, 1, 3)  # [batch_size, n_heads, n_neighbors, memory_dim]\n",
    "        \n",
    "        value = self.w_value(key_input).view(batch_size, n_neighbors, self.n_heads, memory_dim)\n",
    "        value = value.permute(0, 2, 1, 3)  # [batch_size, n_heads, n_neighbors, memory_dim]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(query.unsqueeze(2), key.transpose(-2, -1)) / math.sqrt(memory_dim)\n",
    "        \n",
    "        # Apply decay-based attenuation (with decay_factor=0, this has no effect)\n",
    "        time_decay = torch.exp(-self.decay_factor * time_diffs).unsqueeze(1).unsqueeze(1)\n",
    "        scores = scores * time_decay\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        context = context.transpose(1, 2).reshape(batch_size, -1)\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.output_layer(context)\n",
    "        \n",
    "        # Apply residual connection if dimensions match\n",
    "        if output.size(1) == node_features.size(1):\n",
    "            output = self.layer_norm(output + node_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"DecayTemporalAttention class with disabled decay factor defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60160c5",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset\n",
    "\n",
    "We'll load the Reddit dataset using the same function as in the main notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09932031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data preparation\n",
    "def load_reddit_dataset():\n",
    "    \"\"\"Load and preprocess the Reddit dataset for TGN\"\"\"\n",
    "    try:\n",
    "        # Try to load the dataset\n",
    "        data_path = os.path.join('data', 'reddit_TGAT.csv')\n",
    "        if not os.path.exists(data_path):\n",
    "            data_path = os.path.join('data', 'soc-redditHyperlinks-title.tsv')\n",
    "            if os.path.exists(data_path):\n",
    "                df = pd.read_csv(data_path, sep='\\t')\n",
    "            else:\n",
    "                raise FileNotFoundError(\"Reddit dataset not found\")\n",
    "        else:\n",
    "            df = pd.read_csv(data_path)\n",
    "            \n",
    "        print(f\"Loaded Reddit dataset with {len(df)} edges\")\n",
    "        \n",
    "        # Extract nodes and create a mapping\n",
    "        if 'SOURCE_SUBREDDIT' in df.columns and 'TARGET_SUBREDDIT' in df.columns:\n",
    "            src_col, dst_col = 'SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT'\n",
    "        else:\n",
    "            src_col, dst_col = df.columns[0], df.columns[1]\n",
    "            \n",
    "        all_nodes = pd.concat([df[src_col], df[dst_col]]).unique()\n",
    "        node_mapping = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "        num_nodes = len(node_mapping)\n",
    "        \n",
    "        # Extract timestamps\n",
    "        if 'TIMESTAMP' in df.columns:\n",
    "            time_col = 'TIMESTAMP'\n",
    "            # Convert to numerical format if needed\n",
    "            df['timestamp'] = pd.to_datetime(df[time_col]).astype(int) / 10**9\n",
    "        elif 'timestamp' in df.columns:\n",
    "            time_col = 'timestamp'\n",
    "            df['timestamp'] = df[time_col].astype(float)\n",
    "        else:\n",
    "            # If no timestamp, create artificial ones\n",
    "            df['timestamp'] = range(len(df))\n",
    "            \n",
    "        # Sort by timestamp\n",
    "        df = df.sort_values('timestamp')\n",
    "        \n",
    "        # Map nodes to indices\n",
    "        df['src_idx'] = df[src_col].map(node_mapping)\n",
    "        df['dst_idx'] = df[dst_col].map(node_mapping)\n",
    "        \n",
    "        # Create node features (random for demonstration)\n",
    "        node_features = np.random.randn(num_nodes, 100)  # 100-dimensional features\n",
    "        \n",
    "        # Create edge features\n",
    "        if 'PROPERTIES' in df.columns:\n",
    "            # Use properties as edge features\n",
    "            # This is a placeholder - would need actual feature extraction\n",
    "            edge_features = np.random.randn(len(df), 50)  # 50-dimensional features\n",
    "        else:\n",
    "            # Create random edge features\n",
    "            edge_features = np.random.randn(len(df), 50)  # 50-dimensional features\n",
    "            \n",
    "        # Split into train/val/test\n",
    "        train_ratio, val_ratio = 0.7, 0.15\n",
    "        train_end = int(len(df) * train_ratio)\n",
    "        val_end = int(len(df) * (train_ratio + val_ratio))\n",
    "        \n",
    "        train_df = df.iloc[:train_end]\n",
    "        val_df = df.iloc[train_end:val_end]\n",
    "        test_df = df.iloc[val_end:]\n",
    "        \n",
    "        print(f\"Split dataset into {len(train_df)} train, {len(val_df)} validation, and {len(test_df)} test samples\")\n",
    "        print(f\"Number of unique nodes: {num_nodes}\")\n",
    "        \n",
    "        return {\n",
    "            'train_df': train_df,\n",
    "            'val_df': val_df,\n",
    "            'test_df': test_df,\n",
    "            'node_features': node_features,\n",
    "            'edge_features': edge_features,\n",
    "            'num_nodes': num_nodes\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "dataset_info = load_reddit_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a4302",
   "metadata": {},
   "source": [
    "## 3. Visualize the Dataset\n",
    "\n",
    "Now let's visualize the Reddit dataset as a graph using our new visualization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579aeca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset if available\n",
    "if dataset_info:\n",
    "    G = visualize_reddit_dataset(dataset_info)\n",
    "else:\n",
    "    print(\"Dataset not available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5781428",
   "metadata": {},
   "source": [
    "## 4. Initialize and Use the Modified TGN Model\n",
    "\n",
    "Now we'll initialize the TGN model with the modified DecayTemporalAttention class (decay factor set to 0) for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "if dataset_info:\n",
    "    num_nodes = dataset_info['num_nodes']\n",
    "else:\n",
    "    num_nodes = 1000  # Default if dataset not available\n",
    "\n",
    "# Model hyperparameters\n",
    "node_feat_dim = 100  # Dimension of node features\n",
    "edge_feat_dim = 50   # Dimension of edge features\n",
    "memory_dim = 100     # Dimension of node memory\n",
    "time_dim = 10        # Dimension of time encoding\n",
    "embedding_dim = 100  # Dimension of final node embeddings\n",
    "message_dim = 100    # Dimension of messages\n",
    "n_layers = 2         # Number of graph attention layers\n",
    "n_heads = 2          # Number of attention heads\n",
    "dropout = 0.1        # Dropout probability\n",
    "\n",
    "# Initialize the model\n",
    "model = TemporalGraphNetwork(\n",
    "    num_nodes=num_nodes,\n",
    "    node_feat_dim=node_feat_dim,\n",
    "    edge_feat_dim=edge_feat_dim,\n",
    "    memory_dim=memory_dim,\n",
    "    time_dim=time_dim,\n",
    "    embedding_dim=embedding_dim,\n",
    "    message_dim=message_dim,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    dropout=dropout,\n",
    "    use_memory=True,\n",
    "    message_function='mlp',     # Options: 'mlp', 'identity'\n",
    "    memory_updater='gru',       # Options: 'gru', 'rnn'\n",
    "    aggregator='lstm'           # Options: 'lstm', 'mean'\n",
    ")\n",
    "\n",
    "print(f\"Enhanced TGN model initialized with {n_layers} layers and {n_heads} attention heads.\")\n",
    "print(\"Note: The decay factor in DecayTemporalAttention is set to 0 for evaluation without decay metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f8723",
   "metadata": {},
   "source": [
    "## 5. Comparison with and without Decay Factor\n",
    "\n",
    "For illustration, let's show how the model behaves differently with and without the decay factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc640e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Function to demonstrate how the decay factor affects attention weights\n",
    "def compare_decay_factors():\n",
    "    # Simulate time differences between current time and past interactions\n",
    "    time_diffs = torch.tensor([0.1, 1.0, 5.0, 10.0, 20.0])\n",
    "    \n",
    "    # Calculate decay with different factors\n",
    "    decay_factors = [0.0, 0.1, 0.5, 1.0]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for factor in decay_factors:\n",
    "        decay = torch.exp(-factor * time_diffs)\n",
    "        plt.plot(time_diffs.numpy(), decay.numpy(), marker='o', label=f\"Decay factor = {factor}\")\n",
    "    \n",
    "    plt.title(\"Effect of Decay Factor on Attention Weights\")\n",
    "    plt.xlabel(\"Time Difference\")\n",
    "    plt.ylabel(\"Attention Weight Multiplier\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Explain the impact\n",
    "    print(\"Impact of decay factor on the model:\")\n",
    "    print(\"- With decay_factor = 0.0 (our modified version): All interactions receive equal weight regardless of time.\")\n",
    "    print(\"- With decay_factor > 0: Recent interactions get higher weight than older ones.\")\n",
    "    print(\"- Higher decay factors cause weights to decay more rapidly with time.\")\n",
    "    print(\"\\nBy setting decay_factor = 0, we can evaluate the model's performance without the temporal decay effect.\")\n",
    "\n",
    "# Run the comparison\n",
    "compare_decay_factors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1abf837",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. Modified the DecayTemporalAttention class by setting the decay factor to zero to evaluate the model without temporal decay.\n",
    "2. Added visualization capabilities to view the Reddit dataset as a graph, including community structure and temporal evolution.\n",
    "3. Demonstrated how the decay factor affects the attention weights in the TGN model.\n",
    "\n",
    "These modifications allow for a more comprehensive analysis of the TGN model's behavior and the structure of temporal graph datasets."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
