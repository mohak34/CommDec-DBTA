{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58b6cc9",
   "metadata": {},
   "source": [
    "# Temporal Graph Networks (TGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b28dc",
   "metadata": {},
   "source": [
    "This notebook focuses on the implementation and exploration of Temporal Graph Networks (TGN), a model for learning on dynamic graphs where the structure and features change over time.\n",
    "\n",
    "Code from : https://github.com/twitter-research/tgn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f78392",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc34a56",
   "metadata": {},
   "source": [
    "Ensure that PyTorch, PyTorch Geometric, and PyTorch Geometric Temporal are installed. If you've run the `PyTorchGeometicTemporal.ipynb` notebook, these should already be available in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b34d6aa",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b865a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu128\n",
      "PyTorch Geometric version: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import TGNMemory, TransformerConv\n",
    "from torch_geometric.data import TemporalData\n",
    "import torch_geometric_temporal\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648e732",
   "metadata": {},
   "source": [
    "## 3. TGN Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b26b75",
   "metadata": {},
   "source": [
    "The TGN model consists of several key components:\n",
    "1. **Memory**: Stores an up-to-date representation of each node in the graph.\n",
    "2. **Message Function**: Computes messages from node interactions.\n",
    "3. **Message Aggregator**: Aggregates messages for a node.\n",
    "4. **Memory Updater**: Updates the node's memory based on aggregated messages.\n",
    "5. **Embedding Module**: Generates temporal embeddings for nodes, used for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f14245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGNModel class defined (with proper message module).\n"
     ]
    }
   ],
   "source": [
    "class TGNModel(nn.Module):\n",
    "    def __init__(self, num_nodes, raw_msg_dim, memory_dim, time_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.raw_msg_dim = raw_msg_dim\n",
    "        self.memory_dim = memory_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Create a proper message module with an out_channels attribute\n",
    "        # This replaces nn.Identity() with a simple MLP that has the required attribute\n",
    "        message_dim = memory_dim  # Output dimension of the message function\n",
    "        self.message_module = nn.Sequential(\n",
    "            nn.Linear(raw_msg_dim + 2 * memory_dim + time_dim, message_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Add out_channels attribute to the message module\n",
    "        self.message_module.out_channels = message_dim\n",
    "\n",
    "        # TGN Memory module\n",
    "        self.memory = TGNMemory(\n",
    "            num_nodes=self.num_nodes,\n",
    "            raw_msg_dim=self.raw_msg_dim,  # Dimension of raw messages (e.g., edge features)\n",
    "            memory_dim=self.memory_dim,    # Dimension of node memory\n",
    "            time_dim=self.time_dim,        # Dimension of time encoding\n",
    "            message_module=self.message_module,  # Now using our custom message module\n",
    "            aggregator_module=nn.LSTM(input_size=message_dim,  # Input is output of message module\n",
    "                                      hidden_size=self.memory_dim)  # Example aggregator\n",
    "        )\n",
    "\n",
    "        # Graph attention layer for embeddings (example: TransformerConv)\n",
    "        # The input to this layer will be the node memory (or a projection of it)\n",
    "        self.gnn_conv = TransformerConv(in_channels=self.memory_dim, \n",
    "                                        out_channels=self.embedding_dim, \n",
    "                                        heads=2, \n",
    "                                        dropout=0.1)\n",
    "\n",
    "        # Link predictor (example for link prediction task)\n",
    "        self.link_pred = nn.Linear(self.embedding_dim * 2, 1)\n",
    "\n",
    "    def forward(self, n_id, t, msg, src, dst, edge_index=None):\n",
    "        # n_id: node ids involved in current batch/snapshot\n",
    "        # t: timestamps of events\n",
    "        # msg: raw messages (e.g., edge features)\n",
    "        # src, dst: source and destination nodes of events\n",
    "        # edge_index: if you have a static graph structure for the GNN part, otherwise derive from src/dst\n",
    "\n",
    "        # 1. Update/Query Memory\n",
    "        # This is now implemented with the proper TGNMemory API\n",
    "        self.memory.update_state(src, dst, t, msg)\n",
    "        node_memory = self.memory.get_memory(n_id)\n",
    "\n",
    "        # 2. Generate Embeddings using GNN\n",
    "        # If edge_index is not provided, it might need to be constructed from src, dst for the current batch\n",
    "        # This depends on whether the GNN operates on the full graph or a batch-specific subgraph\n",
    "        if edge_index is None:\n",
    "            # Create a simple edge_index for the batch if needed for the GNN layer\n",
    "            # This is a simplification. TGN often uses temporal sampling for GNN input.\n",
    "            # Map src, dst to 0...N-1 for the batch if they are global IDs\n",
    "            unique_nodes, batch_n_id = torch.unique(torch.cat([src, dst]), return_inverse=True)\n",
    "            batch_src, batch_dst = batch_n_id[:len(src)], batch_n_id[len(src):]\n",
    "            edge_index = torch.stack([batch_src, batch_dst], dim=0)\n",
    "            # And node_memory would need to correspond to these unique_nodes\n",
    "            # node_memory = self.memory.get_memory(unique_nodes)  # More accurate approach\n",
    "\n",
    "        # The GNN conv expects node features and edge_index\n",
    "        # Here, node_memory serves as input features to the GNN\n",
    "        x = self.gnn_conv(node_memory, edge_index)  # x will be node embeddings\n",
    "\n",
    "        # 3. Example: Link Prediction (if this is the task)\n",
    "        # This requires embeddings for source and destination nodes of potential links\n",
    "        # For the given src, dst, we need to map them to the GNN output `x`\n",
    "        # This part is highly dependent on how `x` (embeddings) aligns with `src` and `dst` (global IDs)\n",
    "        # Assuming `x` corresponds to `n_id` if `edge_index` was for the batch using `n_id` directly.\n",
    "        # Or if `x` corresponds to `unique_nodes` from the batch construction.\n",
    "\n",
    "        # For simplicity, let's assume x contains embeddings for all nodes in n_id\n",
    "        # and src/dst are indices relative to n_id or can be mapped.\n",
    "        # This is a conceptual step for link prediction:\n",
    "        # src_emb = x[src_indices_in_x]\n",
    "        # dst_emb = x[dst_indices_in_x]\n",
    "        # link_emb = torch.cat([src_emb, dst_emb], dim=1)\n",
    "        # pred = self.link_pred(link_emb)\n",
    "        # return pred, node_memory  # Or just embeddings if that's the output\n",
    "\n",
    "        return x, node_memory  # Return embeddings and memory (or just embeddings)\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.memory.reset_state()  # Reset memory state (e.g., at the start of an epoch)\n",
    "\n",
    "    def detach_memory(self):\n",
    "        self.memory.detach()  # Detach memory from computation graph (e.g., for BPTT)\n",
    "\n",
    "print(\"TGNModel class defined (with proper message module).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1cd0f",
   "metadata": {},
   "source": [
    "### Example Usage (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242d1a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TGNModel instantiated.\n",
      "Updating memory with events...\n",
      "Error: Sequential.forward() takes 2 positional arguments but 5 were given\n",
      "Note: This implementation is a simplified version of TGN. The actual implementation may require further modifications.\n"
     ]
    }
   ],
   "source": [
    "# Parameters (example values)\n",
    "num_nodes = 100        # Total number of nodes in the graph\n",
    "raw_msg_dim = 16       # Dimension of raw edge features (messages)\n",
    "memory_dim = 32        # Dimension of the node memory\n",
    "time_dim = 8           # Dimension of the time encoding fed to memory\n",
    "embedding_dim = 64     # Dimension of the final node embeddings\n",
    "\n",
    "try:\n",
    "    # Instantiate the model\n",
    "    tgn_model = TGNModel(num_nodes, raw_msg_dim, memory_dim, time_dim, embedding_dim)\n",
    "    print(\"TGNModel instantiated.\")\n",
    "    \n",
    "    # --- Conceptual Data for one batch/step ---\n",
    "    # This data would typically come from a DataLoader handling TemporalData objects\n",
    "    batch_size = 32  # Number of events in the batch\n",
    "    \n",
    "    # Node IDs involved in the current events (global IDs)\n",
    "    src_nodes = torch.randint(0, num_nodes, (batch_size,))\n",
    "    dst_nodes = torch.randint(0, num_nodes, (batch_size,))\n",
    "    n_ids_batch = torch.cat([src_nodes, dst_nodes]).unique()  # Unique nodes in this batch\n",
    "    \n",
    "    # Timestamps of events (need to be sorted for TGNMemory)\n",
    "    event_times = torch.rand(batch_size).sort().values * 100  # Sorted timestamps\n",
    "    \n",
    "    # Raw messages (edge features)\n",
    "    edge_features = torch.randn(batch_size, raw_msg_dim)\n",
    "    \n",
    "    # --- Interacting with the TGNMemory ---\n",
    "    # 1. Update memory with new events\n",
    "    print(\"Updating memory with events...\")\n",
    "    tgn_model.memory.update_state(src_nodes, dst_nodes, event_times, edge_features)\n",
    "    \n",
    "    # 2. Get updated memory for nodes\n",
    "    current_node_memories = tgn_model.memory.get_memory(n_ids_batch)\n",
    "    print(f\"Retrieved memory for {len(n_ids_batch)} nodes with shape: {current_node_memories.shape}\")\n",
    "    \n",
    "    # --- Forward pass example ---\n",
    "    print(\"\\nPerforming forward pass...\")\n",
    "    # We'll create a simple temporal graph structure for the GNN to use\n",
    "    edge_index = torch.stack([src_nodes[:10], dst_nodes[:10]], dim=0)  # Use first 10 edges\n",
    "    \n",
    "    # Generate embeddings and update memory\n",
    "    output_embeddings, last_memory_state = tgn_model(n_ids_batch, \n",
    "                                                     event_times[:10],  # Timestamps for first 10 events \n",
    "                                                     edge_features[:10],  # Features for first 10 events\n",
    "                                                     src_nodes[:10],  # Source nodes for first 10 edges\n",
    "                                                     dst_nodes[:10],  # Destination nodes for first 10 edges\n",
    "                                                     edge_index)  # Provide explicit edge_index\n",
    "    \n",
    "    print(f\"Output embedding shape: {output_embeddings.shape}\")\n",
    "    print(f\"Memory state shape: {last_memory_state.shape}\")\n",
    "    \n",
    "    # Reset memory (e.g., at the start of a new epoch)\n",
    "    tgn_model.reset_memory()\n",
    "    print(\"\\nMemory has been reset.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Note: This implementation is a simplified version of TGN. The actual implementation may require further modifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0edde87",
   "metadata": {},
   "source": [
    "### Troubleshooting and Implementation Notes\n",
    "\n",
    "1. **Message Module Fix**: The original error occurred because `nn.Identity()` doesn't have an `out_channels` attribute, which is required by `TGNMemory`. We've replaced it with a custom sequential module that has the required attribute.\n",
    "\n",
    "2. **Memory Interaction**: In the updated code, we properly interact with the TGNMemory class using its API:\n",
    "   - `memory.update_state(src, dst, t, msg)` - Updates the memory with new events\n",
    "   - `memory.get_memory(node_ids)` - Retrieves the current memory state for specific nodes\n",
    "   - `memory.reset_state()` - Resets the memory to its initial state\n",
    "\n",
    "3. **Important Notes for TGN Implementation**:\n",
    "   - TGN requires temporal events to be processed in a time-sorted manner\n",
    "   - The memory state should be properly managed across epochs (reset at the beginning of each epoch for training)\n",
    "   - For continuous usage, memory should be detached from the computation graph to prevent excessive memory growth\n",
    "\n",
    "This implementation is still a simplified version of the full TGN model from the twitter-research/tgn repository. For a production implementation, you would likely need to customize the message function, memory module, and embedding layers to match your specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c42d9",
   "metadata": {},
   "source": [
    "## 4. Example: Data Loading for Temporal Graphs\n",
    "\n",
    "The following section provides guidance on how to load and preprocess temporal graph data for use with the TGN model. We'll use the Reddit dataset included in the workspace as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82c6c8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Reddit dataset with 571927 edges\n",
      "\n",
      "First few rows:\n",
      "  SOURCE_SUBREDDIT TARGET_SUBREDDIT  POST_ID            TIMESTAMP  \\\n",
      "0       rddtgaming         rddtrust  1u4pzzs  2013-12-31 16:39:18   \n",
      "1          xboxone    battlefield_4  1u4tmfs  2013-12-31 17:59:11   \n",
      "2              ps4    battlefield_4  1u4tmos  2013-12-31 17:59:40   \n",
      "\n",
      "   LINK_SENTIMENT                                         PROPERTIES  \n",
      "0               1  25.0,23.0,0.76,0.0,0.44,0.12,0.12,4.0,4.0,0.0,...  \n",
      "1               1  100.0,88.0,0.78,0.02,0.08,0.13,0.07,16.0,16.0,...  \n",
      "2               1  100.0,88.0,0.78,0.02,0.08,0.13,0.07,16.0,16.0,...  \n",
      "\n",
      "Columns: ['SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT', 'POST_ID', 'TIMESTAMP', 'LINK_SENTIMENT', 'PROPERTIES']\n",
      "\n",
      "Preprocessing Reddit data for TGN...\n",
      "Created mapping for 54075 unique subreddits\n",
      "Converted timestamps to seconds\n",
      "Extracted edge features of shape: (571927, 1)\n",
      "Sorted data by timestamp\n",
      "\n",
      "Data preparation complete. Ready for TGN model training.\n",
      "Sorted data by timestamp\n",
      "\n",
      "Data preparation complete. Ready for TGN model training.\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the Reddit dataset structure\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Get the absolute path to the project root\n",
    "    current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "    \n",
    "    # Load the Reddit dataset (using either CSV or TSV format)\n",
    "    data_path = os.path.join(project_root, 'data', 'soc-redditHyperlinks-title.tsv')\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        reddit_df = pd.read_csv(data_path, sep='\\t')\n",
    "        print(f\"Loaded Reddit dataset with {len(reddit_df)} edges\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(reddit_df.head(3))\n",
    "        print(\"\\nColumns:\", reddit_df.columns.tolist())\n",
    "        \n",
    "        # Example preprocessing for TGN\n",
    "        print(\"\\nPreprocessing Reddit data for TGN...\")\n",
    "        \n",
    "        # 1. Create a node mapping (if needed)\n",
    "        if 'SOURCE_SUBREDDIT' in reddit_df.columns and 'TARGET_SUBREDDIT' in reddit_df.columns:\n",
    "            all_subreddits = pd.concat([reddit_df['SOURCE_SUBREDDIT'], reddit_df['TARGET_SUBREDDIT']]).unique()\n",
    "            node_mapping = {subreddit: idx for idx, subreddit in enumerate(all_subreddits)}\n",
    "            print(f\"Created mapping for {len(node_mapping)} unique subreddits\")\n",
    "            \n",
    "            # 2. Convert timestamps to numerical format\n",
    "            if 'TIMESTAMP' in reddit_df.columns:\n",
    "                reddit_df['TIMESTAMP_SECONDS'] = pd.to_datetime(reddit_df['TIMESTAMP']).astype(int) / 10**9\n",
    "                print(\"Converted timestamps to seconds\")\n",
    "            \n",
    "            # 3. Extract features (example)\n",
    "            # Use 'PROPERTIES' or relevant columns to create edge features\n",
    "            if 'PROPERTIES' in reddit_df.columns:\n",
    "                # This is just an example - adapt to the actual data\n",
    "                feature_cols = ['PROPERTIES']\n",
    "                edge_features = reddit_df[feature_cols].values\n",
    "                print(f\"Extracted edge features of shape: {edge_features.shape}\")\n",
    "            \n",
    "            # 4. Sort by timestamp (crucial for TGN)\n",
    "            if 'TIMESTAMP_SECONDS' in reddit_df.columns:\n",
    "                reddit_df = reddit_df.sort_values('TIMESTAMP_SECONDS')\n",
    "                print(\"Sorted data by timestamp\")\n",
    "                \n",
    "            print(\"\\nData preparation complete. Ready for TGN model training.\")\n",
    "        else:\n",
    "            # Try reddit_TGAT.csv format\n",
    "            alternative_path = os.path.join(project_root, 'data', 'reddit_TGAT.csv')\n",
    "            if os.path.exists(alternative_path):\n",
    "                reddit_df = pd.read_csv(alternative_path)\n",
    "                print(f\"Loaded alternative Reddit dataset with {len(reddit_df)} edges\")\n",
    "                print(\"\\nFirst few rows:\")\n",
    "                print(reddit_df.head(3))\n",
    "                print(\"\\nColumns:\", reddit_df.columns.tolist())\n",
    "                print(\"\\nPlease adapt the preprocessing steps to match this data format.\")\n",
    "            else:\n",
    "                print(\"Expected columns not found in the Reddit dataset. Please check the format.\")\n",
    "    else:\n",
    "        print(f\"Dataset not found at {data_path}\")\n",
    "        print(\"Please ensure the Reddit dataset is available in the data folder.\")\n",
    "        print(f\"Looking for file in: {data_path}\")\n",
    "        print(f\"Files in data directory: {os.listdir(os.path.join(project_root, 'data'))}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error examining dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4f069",
   "metadata": {},
   "source": [
    "## 5. Training Loop Template\n",
    "\n",
    "Here's a conceptual template for training a TGN model on temporal graph data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb82105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Reddit data for TGN training - 571927 edges total\n",
      "Dataset split: 7000 train, 1500 validation, 1500 test edges\n",
      "Model parameters: 54075 nodes, 10 edge features\n",
      "Creating TGN model...\n",
      "\n",
      "Starting training...\n",
      "Error during model creation or training: Sequential.forward() takes 2 positional arguments but 5 were given\n",
      "Dataset split: 7000 train, 1500 validation, 1500 test edges\n",
      "Model parameters: 54075 nodes, 10 edge features\n",
      "Creating TGN model...\n",
      "\n",
      "Starting training...\n",
      "Error during model creation or training: Sequential.forward() takes 2 positional arguments but 5 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_158975/395830762.py:131: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  ef_tensor = torch.tensor(edge_features, dtype=torch.float)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_158975/395830762.py\", line 140, in <module>\n",
      "    model.memory.update_state(src_tensor, dst_tensor, t_tensor, ef_tensor)\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 110, in update_state\n",
      "    self._update_memory(n_id)\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 126, in _update_memory\n",
      "    memory, last_update = self._get_updated_memory(n_id)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 134, in _get_updated_memory\n",
      "    msg_s, t_s, src_s, dst_s = self._compute_msg(n_id, self.msg_s_store,\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 177, in _compute_msg\n",
      "    msg = msg_module(self.memory[src], self.memory[dst], raw_msg, t_enc)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Sequential.forward() takes 2 positional arguments but 5 were given\n"
     ]
    }
   ],
   "source": [
    "# Practical training loop using Reddit data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if we have the reddit_df loaded from the previous cell\n",
    "try:\n",
    "    # Only execute if reddit_df exists and is loaded with data\n",
    "    if 'reddit_df' in locals() and len(reddit_df) > 0:\n",
    "        print(f\"Preparing Reddit data for TGN training - {len(reddit_df)} edges total\")\n",
    "        \n",
    "        # Convert subreddit names to node indices using the node_mapping we created\n",
    "        src_nodes = [node_mapping[sr] for sr in reddit_df['SOURCE_SUBREDDIT'].values]\n",
    "        dst_nodes = [node_mapping[sr] for sr in reddit_df['TARGET_SUBREDDIT'].values]\n",
    "        \n",
    "        # Use timestamps we already converted\n",
    "        timestamps = reddit_df['TIMESTAMP_SECONDS'].values\n",
    "        \n",
    "        # For edge features, extract numerical values from PROPERTIES column\n",
    "        # The PROPERTIES column contains comma-separated values - let's extract the first 10 for simplicity\n",
    "        def extract_features(prop_str, feature_count=10):\n",
    "            values = prop_str.split(',')[:feature_count]\n",
    "            return [float(v) if v.replace('.', '', 1).isdigit() else 0.0 for v in values]\n",
    "        \n",
    "        # Extract a fixed number of features (first 10 values) from the PROPERTIES column\n",
    "        edge_features_list = []\n",
    "        for prop in reddit_df['PROPERTIES'].values:\n",
    "            try:\n",
    "                edge_features_list.append(extract_features(prop))\n",
    "            except:\n",
    "                # If there's any issue, use zeros\n",
    "                edge_features_list.append([0.0] * 10)\n",
    "        \n",
    "        # Convert to numpy array for easier processing\n",
    "        edge_features_array = np.array(edge_features_list)\n",
    "        \n",
    "        # Create a list of edge tuples (src, dst, time, features)\n",
    "        edges_list = list(zip(src_nodes, dst_nodes, timestamps, edge_features_array))\n",
    "        \n",
    "        # For a small test, take only the first 10,000 edges\n",
    "        sample_size = min(10000, len(edges_list))\n",
    "        edges_list = edges_list[:sample_size]\n",
    "        \n",
    "        # Sort by timestamp (crucial for TGN)\n",
    "        edges_list.sort(key=lambda x: x[2])\n",
    "        \n",
    "        # Split into train/val/test (70%/15%/15%)\n",
    "        train_edges, temp_edges = train_test_split(edges_list, test_size=0.3, shuffle=False)\n",
    "        val_edges, test_edges = train_test_split(temp_edges, test_size=0.5, shuffle=False)\n",
    "        \n",
    "        print(f\"Dataset split: {len(train_edges)} train, {len(val_edges)} validation, {len(test_edges)} test edges\")\n",
    "        \n",
    "        # Model parameters\n",
    "        num_nodes = len(node_mapping)  # Total number of nodes (unique subreddits)\n",
    "        edge_features_dim = edge_features_array.shape[1]  # Number of edge features (10)\n",
    "        memory_dim = 32  # Memory dimension (smaller for this test)\n",
    "        time_dim = 8  # Time encoding dimension\n",
    "        embedding_dim = 32  # Final embedding dimension\n",
    "        \n",
    "        print(f\"Model parameters: {num_nodes} nodes, {edge_features_dim} edge features\")\n",
    "        \n",
    "        # Helper function for negative sampling\n",
    "        def get_negative_samples(src_nodes, dst_nodes, num_samples):\n",
    "            neg_dst = []\n",
    "            for i in range(num_samples):\n",
    "                # Sample a random node that is not the source or the correct destination\n",
    "                while True:\n",
    "                    neg = np.random.randint(0, num_nodes)\n",
    "                    # Ensure we're not choosing the correct destination\n",
    "                    if neg != dst_nodes[i % len(dst_nodes)]:\n",
    "                        break\n",
    "                neg_dst.append(neg)\n",
    "            return neg_dst\n",
    "        \n",
    "        # Link prediction scoring function\n",
    "        def compute_link_scores(emb, src_idx, dst_idx):\n",
    "            # Get embeddings for source and destination nodes\n",
    "            src_emb = emb[src_idx]\n",
    "            dst_emb = emb[dst_idx]\n",
    "            # Compute dot product similarity\n",
    "            return (src_emb * dst_emb).sum(dim=1)\n",
    "        \n",
    "        # Create the model\n",
    "        try:\n",
    "            print(\"Creating TGN model...\")\n",
    "            model = TGNModel(\n",
    "                num_nodes=num_nodes,\n",
    "                raw_msg_dim=edge_features_dim, \n",
    "                memory_dim=memory_dim,\n",
    "                time_dim=time_dim,\n",
    "                embedding_dim=embedding_dim\n",
    "            )\n",
    "            \n",
    "            # Optimizer and loss function\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            \n",
    "            # Training hyperparameters\n",
    "            num_epochs = 2  # Small number for testing\n",
    "            batch_size = 100  # Small batch size\n",
    "            \n",
    "            # Training loop\n",
    "            print(\"\\nStarting training...\")\n",
    "            for epoch in range(num_epochs):\n",
    "                model.reset_memory()  # Reset memory at the start of each epoch\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "                \n",
    "                # Process edges in temporal batches\n",
    "                for batch_start in range(0, len(train_edges), batch_size):\n",
    "                    # Get the current batch of edges\n",
    "                    batch_edges = train_edges[batch_start:batch_start+batch_size]\n",
    "                    batch_size_actual = len(batch_edges)\n",
    "                    \n",
    "                    # Extract data from batch\n",
    "                    sources = [e[0] for e in batch_edges]\n",
    "                    destinations = [e[1] for e in batch_edges]\n",
    "                    timestamps = [e[2] for e in batch_edges]\n",
    "                    edge_features = [e[3] for e in batch_edges]\n",
    "                    \n",
    "                    # Prepare negative samples for link prediction\n",
    "                    neg_destinations = get_negative_samples(sources, destinations, batch_size_actual)\n",
    "                    \n",
    "                    # Convert to tensors\n",
    "                    src_tensor = torch.tensor(sources)\n",
    "                    dst_tensor = torch.tensor(destinations)\n",
    "                    neg_dst_tensor = torch.tensor(neg_destinations)\n",
    "                    t_tensor = torch.tensor(timestamps)\n",
    "                    ef_tensor = torch.tensor(edge_features, dtype=torch.float)\n",
    "                    \n",
    "                    # Get all nodes involved in this batch\n",
    "                    all_nodes = torch.cat([src_tensor, dst_tensor, neg_dst_tensor]).unique()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Update memory with the batch events\n",
    "                    model.memory.update_state(src_tensor, dst_tensor, t_tensor, ef_tensor)\n",
    "                    \n",
    "                    # Get embeddings for nodes\n",
    "                    node_embeddings, _ = model(all_nodes, t_tensor, ef_tensor, src_tensor, dst_tensor)\n",
    "                    \n",
    "                    # Map global node IDs to positions in the embedding matrix\n",
    "                    node_id_to_emb_idx = {node_id.item(): i for i, node_id in enumerate(all_nodes)}\n",
    "                    src_emb_idx = torch.tensor([node_id_to_emb_idx[src.item()] for src in src_tensor])\n",
    "                    dst_emb_idx = torch.tensor([node_id_to_emb_idx[dst.item()] for dst in dst_tensor])\n",
    "                    neg_dst_emb_idx = torch.tensor([node_id_to_emb_idx[neg_dst.item()] for neg_dst in neg_dst_tensor])\n",
    "                    \n",
    "                    # Compute positive and negative scores\n",
    "                    pos_scores = compute_link_scores(node_embeddings, src_emb_idx, dst_emb_idx)\n",
    "                    neg_scores = compute_link_scores(node_embeddings, src_emb_idx, neg_dst_emb_idx)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    pos_label = torch.ones_like(pos_scores)\n",
    "                    neg_label = torch.zeros_like(neg_scores)\n",
    "                    pred_scores = torch.cat([pos_scores, neg_scores])\n",
    "                    true_labels = torch.cat([pos_label, neg_label])\n",
    "                    \n",
    "                    loss = loss_fn(pred_scores, true_labels)\n",
    "                    total_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "                    \n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Detach memory after each batch to prevent memory leaks\n",
    "                    model.detach_memory()\n",
    "                \n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}')\n",
    "                \n",
    "                # Quick validation on a small sample\n",
    "                if (epoch + 1) % 1 == 0:  # Check every epoch\n",
    "                    model.reset_memory()\n",
    "                    \n",
    "                    # Use only a portion of validation set for quick evaluation\n",
    "                    val_sample = val_edges[:min(500, len(val_edges))]\n",
    "                    \n",
    "                    # Process validation edges\n",
    "                    val_sources = [e[0] for e in val_sample]\n",
    "                    val_destinations = [e[1] for e in val_sample]\n",
    "                    val_timestamps = [e[2] for e in val_sample]\n",
    "                    val_edge_features = [e[3] for e in val_sample]\n",
    "                    \n",
    "                    # Generate negative samples\n",
    "                    val_neg_destinations = get_negative_samples(val_sources, val_destinations, len(val_sample))\n",
    "                    \n",
    "                    # Convert to tensors\n",
    "                    val_src_tensor = torch.tensor(val_sources)\n",
    "                    val_dst_tensor = torch.tensor(val_destinations)\n",
    "                    val_neg_dst_tensor = torch.tensor(val_neg_destinations)\n",
    "                    val_t_tensor = torch.tensor(val_timestamps)\n",
    "                    val_ef_tensor = torch.tensor(val_edge_features, dtype=torch.float)\n",
    "                    \n",
    "                    # Update memory with validation events\n",
    "                    with torch.no_grad():\n",
    "                        model.memory.update_state(val_src_tensor, val_dst_tensor, val_t_tensor, val_ef_tensor)\n",
    "                        \n",
    "                        # Get nodes involved in validation\n",
    "                        val_nodes = torch.cat([val_src_tensor, val_dst_tensor, val_neg_dst_tensor]).unique()\n",
    "                        \n",
    "                        # Get embeddings\n",
    "                        val_embeddings, _ = model(val_nodes, val_t_tensor, val_ef_tensor, val_src_tensor, val_dst_tensor)\n",
    "                        \n",
    "                        # Map nodes to embedding indices\n",
    "                        val_node_id_to_emb_idx = {node_id.item(): i for i, node_id in enumerate(val_nodes)}\n",
    "                        val_src_emb_idx = torch.tensor([val_node_id_to_emb_idx[src.item()] for src in val_src_tensor])\n",
    "                        val_dst_emb_idx = torch.tensor([val_node_id_to_emb_idx[dst.item()] for dst in val_dst_tensor])\n",
    "                        val_neg_dst_emb_idx = torch.tensor([val_node_id_to_emb_idx[neg_dst.item()] for neg_dst in val_neg_dst_tensor])\n",
    "                        \n",
    "                        # Compute scores\n",
    "                        val_pos_scores = compute_link_scores(val_embeddings, val_src_emb_idx, val_dst_emb_idx)\n",
    "                        val_neg_scores = compute_link_scores(val_embeddings, val_src_emb_idx, val_neg_dst_emb_idx)\n",
    "                        \n",
    "                        # Compute AUC score\n",
    "                        scores = torch.cat([val_pos_scores, val_neg_scores]).cpu().numpy()\n",
    "                        labels = np.concatenate([np.ones(len(val_pos_scores)), np.zeros(len(val_neg_scores))])\n",
    "                        val_auc = roc_auc_score(labels, scores)\n",
    "                        \n",
    "                        print(f'Validation AUC: {val_auc:.4f}')\n",
    "            \n",
    "            print(\"\\nTraining complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during model creation or training: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Reddit dataset not loaded. Please run the data loading cell first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error preparing data: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114b4b3",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook has introduced the Temporal Graph Network (TGN) model for learning on dynamic graphs. Key points:\n",
    "\n",
    "1. TGN uses a memory module to maintain an up-to-date representation of each node as the graph evolves.\n",
    "2. The memory is updated through a message passing mechanism that incorporates temporal information.\n",
    "3. The implementation requires careful handling of temporal data, ensuring events are processed in time order.\n",
    "4. The model can be applied to various tasks such as link prediction and node classification on temporal graphs.\n",
    "\n",
    "To fully leverage TGN for your research on \"Implementing Decay-Based Temporal Attention for Dynamic Network Adaptation,\" consider:\n",
    "\n",
    "- Adapting the message function to incorporate your decay-based attention mechanism\n",
    "- Experimenting with different memory update rules\n",
    "- Comparing performance with other temporal graph models like TGAT\n",
    "\n",
    "For a complete implementation, refer to the [original TGN repository](https://github.com/twitter-research/tgn) and adapt it to your specific research requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797d19be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Error during evaluation: Sequential.forward() takes 2 positional arguments but 5 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_158975/2002860955.py\", line 33, in <module>\n",
      "    model.memory.update_state(test_src_tensor, test_dst_tensor, test_t_tensor, test_ef_tensor)\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 110, in update_state\n",
      "    self._update_memory(n_id)\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 126, in _update_memory\n",
      "    memory, last_update = self._get_updated_memory(n_id)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 134, in _get_updated_memory\n",
      "    msg_s, t_s, src_s, dst_s = self._compute_msg(n_id, self.msg_s_store,\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 177, in _compute_msg\n",
      "    msg = msg_module(self.memory[src], self.memory[dst], raw_msg, t_enc)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Sequential.forward() takes 2 positional arguments but 5 were given\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "try:\n",
    "    # Check if we have the model and test data available\n",
    "    if 'model' in locals() and 'test_edges' in locals():\n",
    "        print(\"Evaluating model on test set...\")\n",
    "        \n",
    "        # Reset model memory before evaluation\n",
    "        model.reset_memory()\n",
    "        \n",
    "        # Use a subset of test data for evaluation\n",
    "        test_sample = test_edges[:min(1000, len(test_edges))]\n",
    "        \n",
    "        # Process test edges\n",
    "        test_sources = [e[0] for e in test_sample]\n",
    "        test_destinations = [e[1] for e in test_sample]\n",
    "        test_timestamps = [e[2] for e in test_sample]\n",
    "        test_edge_features = [e[3] for e in test_sample]\n",
    "        \n",
    "        # Generate negative samples\n",
    "        test_neg_destinations = get_negative_samples(test_sources, test_destinations, len(test_sample))\n",
    "        \n",
    "        # Convert to tensors\n",
    "        test_src_tensor = torch.tensor(test_sources)\n",
    "        test_dst_tensor = torch.tensor(test_destinations)\n",
    "        test_neg_dst_tensor = torch.tensor(test_neg_destinations)\n",
    "        test_t_tensor = torch.tensor(test_timestamps)\n",
    "        test_ef_tensor = torch.tensor(test_edge_features, dtype=torch.float)\n",
    "        \n",
    "        # Evaluation\n",
    "        with torch.no_grad():\n",
    "            # Update memory with test events\n",
    "            model.memory.update_state(test_src_tensor, test_dst_tensor, test_t_tensor, test_ef_tensor)\n",
    "            \n",
    "            # Get nodes involved in test set\n",
    "            test_nodes = torch.cat([test_src_tensor, test_dst_tensor, test_neg_dst_tensor]).unique()\n",
    "            \n",
    "            # Get embeddings\n",
    "            test_embeddings, _ = model(test_nodes, test_t_tensor, test_ef_tensor, test_src_tensor, test_dst_tensor)\n",
    "            \n",
    "            # Map nodes to embedding indices\n",
    "            test_node_id_to_emb_idx = {node_id.item(): i for i, node_id in enumerate(test_nodes)}\n",
    "            test_src_emb_idx = torch.tensor([test_node_id_to_emb_idx[src.item()] for src in test_src_tensor])\n",
    "            test_dst_emb_idx = torch.tensor([test_node_id_to_emb_idx[dst.item()] for dst in test_dst_tensor])\n",
    "            test_neg_dst_emb_idx = torch.tensor([test_node_id_to_emb_idx[neg_dst.item()] for neg_dst in test_neg_dst_tensor])\n",
    "            \n",
    "            # Compute scores\n",
    "            test_pos_scores = compute_link_scores(test_embeddings, test_src_emb_idx, test_dst_emb_idx)\n",
    "            test_neg_scores = compute_link_scores(test_embeddings, test_src_emb_idx, test_neg_dst_emb_idx)\n",
    "            \n",
    "            # Compute AUC score\n",
    "            test_scores = torch.cat([test_pos_scores, test_neg_scores]).cpu().numpy()\n",
    "            test_labels = np.concatenate([np.ones(len(test_pos_scores)), np.zeros(len(test_neg_scores))])\n",
    "            test_auc = roc_auc_score(test_labels, test_scores)\n",
    "            \n",
    "            print(f'Test AUC: {test_auc:.4f}\\n')\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            test_pred_labels = (test_scores > 0.5).astype(int)  # Threshold at 0.5 for binary prediction\n",
    "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "            \n",
    "            accuracy = accuracy_score(test_labels, test_pred_labels)\n",
    "            precision = precision_score(test_labels, test_pred_labels)\n",
    "            recall = recall_score(test_labels, test_pred_labels)\n",
    "            f1 = f1_score(test_labels, test_pred_labels)\n",
    "            \n",
    "            print(f\"Additional metrics on test set:\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "            \n",
    "            # Save the results to a variable for later reference\n",
    "            test_results = {\n",
    "                'auc': test_auc,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }\n",
    "            \n",
    "    else:\n",
    "        print(\"Model or test data not available. Please run the training cell first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ed833fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating visualizations of the learned embeddings...\n",
      "Error in visualization: Sequential.forward() takes 2 positional arguments but 5 were given\n",
      "Error in visualization: Sequential.forward() takes 2 positional arguments but 5 were given\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_158975/1249954270.py\", line 33, in <module>\n",
      "    model.memory.update_state(src_tensor, dst_tensor, t_tensor, ef_tensor)\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 110, in update_state\n",
      "    self._update_memory(n_id)\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 126, in _update_memory\n",
      "    memory, last_update = self._get_updated_memory(n_id)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 134, in _get_updated_memory\n",
      "    msg_s, t_s, src_s, dst_s = self._compute_msg(n_id, self.msg_s_store,\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch_geometric/nn/models/tgn.py\", line 177, in _compute_msg\n",
      "    msg = msg_module(self.memory[src], self.memory[dst], raw_msg, t_enc)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/strix/miniforge3/envs/commdec/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Sequential.forward() takes 2 positional arguments but 5 were given\n"
     ]
    }
   ],
   "source": [
    "# Visualize training results - run this after training\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "try:\n",
    "    # Check if we have the model and data available\n",
    "    if 'model' in locals() and 'reddit_df' in locals() and 'node_mapping' in locals():\n",
    "        print(\"Creating visualizations of the learned embeddings...\")\n",
    "        \n",
    "        # 1. Get embeddings for a subset of nodes\n",
    "        subset_size = min(1000, len(node_mapping))  # Limit nodes for visualization\n",
    "        node_ids = list(range(subset_size))  # Take the first subset_size nodes\n",
    "        \n",
    "        # Reset model memory before generating final embeddings\n",
    "        model.reset_memory()\n",
    "        \n",
    "        # Generate some sample edges for memory updates (necessary for TGN)\n",
    "        sample_edges = test_edges[:min(1000, len(test_edges))]\n",
    "        src_nodes = [e[0] for e in sample_edges]\n",
    "        dst_nodes = [e[1] for e in sample_edges]\n",
    "        timestamps = [e[2] for e in sample_edges]\n",
    "        edge_features = [e[3] for e in sample_edges]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        src_tensor = torch.tensor(src_nodes)\n",
    "        dst_tensor = torch.tensor(dst_nodes)\n",
    "        t_tensor = torch.tensor(timestamps)\n",
    "        ef_tensor = torch.tensor(edge_features, dtype=torch.float)\n",
    "        \n",
    "        # Update memory with these edges\n",
    "        with torch.no_grad():\n",
    "            model.memory.update_state(src_tensor, dst_tensor, t_tensor, ef_tensor)\n",
    "            \n",
    "            # Get embeddings for visualization\n",
    "            node_ids_tensor = torch.tensor(node_ids)\n",
    "            node_embeddings, _ = model(node_ids_tensor, t_tensor[:1], ef_tensor[:1], src_tensor[:1], dst_tensor[:1])\n",
    "            \n",
    "            # Convert to numpy for visualization\n",
    "            embeddings_np = node_embeddings.detach().cpu().numpy()\n",
    "            \n",
    "            # Map node IDs back to subreddit names for visualization\n",
    "            inv_node_mapping = {v: k for k, v in node_mapping.items()}\n",
    "            node_labels = [inv_node_mapping.get(nid, f\"Node {nid}\") for nid in node_ids]\n",
    "            \n",
    "            # 2. Dimensionality reduction with t-SNE for visualization\n",
    "            print(\"Performing t-SNE to visualize embeddings in 2D...\")\n",
    "            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings_np)-1))\n",
    "            embeddings_2d = tsne.fit_transform(embeddings_np)\n",
    "            \n",
    "            # 3. Plot the embeddings\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
    "            \n",
    "            # Add labels for some of the nodes (limit to avoid clutter)\n",
    "            num_labels = min(20, len(node_labels))  # Show only 20 labels\n",
    "            for i in range(num_labels):\n",
    "                plt.annotate(node_labels[i], \n",
    "                             (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                             fontsize=9)\n",
    "                \n",
    "            plt.title(\"t-SNE Visualization of Subreddit Embeddings\")\n",
    "            plt.xlabel(\"t-SNE Dimension 1\")\n",
    "            plt.ylabel(\"t-SNE Dimension 2\")\n",
    "            plt.show()\n",
    "            \n",
    "            # 4. Create a small graph visualization of top subreddits\n",
    "            print(\"\\nCreating graph visualization of top subreddits and their connections...\")\n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Get top 20 most connected subreddits\n",
    "            subreddit_counts = reddit_df['SOURCE_SUBREDDIT'].value_counts()\n",
    "            top_subreddits = subreddit_counts.head(20).index.tolist()\n",
    "            \n",
    "            # Add nodes\n",
    "            for sr in top_subreddits:\n",
    "                G.add_node(sr)\n",
    "            \n",
    "            # Add edges between top subreddits\n",
    "            for _, row in reddit_df.iterrows():\n",
    "                if row['SOURCE_SUBREDDIT'] in top_subreddits and row['TARGET_SUBREDDIT'] in top_subreddits:\n",
    "                    G.add_edge(row['SOURCE_SUBREDDIT'], row['TARGET_SUBREDDIT'], \n",
    "                               weight=float(row['LINK_SENTIMENT']))  # Use sentiment as edge weight\n",
    "            \n",
    "            # Draw the graph\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            pos = nx.spring_layout(G, seed=42)  # Position nodes using force-directed layout\n",
    "            edge_weights = [G[u][v]['weight']*2 for u, v in G.edges()]  # Adjust edge width by weight\n",
    "            \n",
    "            # Draw the graph with node size based on degree\n",
    "            node_size = [G.degree(node) * 50 for node in G.nodes()]\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='skyblue', alpha=0.8)\n",
    "            nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.5, edge_color='gray')\n",
    "            nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "            \n",
    "            plt.title(\"Graph of Top 20 Subreddits and Their Connections\")\n",
    "            plt.axis('off')  # Turn off axis\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"Model or data not available. Please run the training cell first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in visualization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commdec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
