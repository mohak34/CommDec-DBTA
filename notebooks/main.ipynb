{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602a5515",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation\n",
    "\n",
    "Train the model and evaluate performance on link prediction and community detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce99eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001, patience=10):\n",
    "    \"\"\"Train the TGN model with early stopping\"\"\"\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    best_val_auc = 0.0\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_aucs = []\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                batch['src_idx'], batch['dst_idx'], batch['timestamp'],\n",
    "                batch['src_features'], batch['dst_features'], batch['edge_features']\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs['link_prob'], batch['label'])\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.extend(outputs['link_prob'].cpu().detach().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_auc = roc_auc_score(all_labels, all_preds)\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = model(\n",
    "                    batch['src_idx'], batch['dst_idx'], batch['timestamp'],\n",
    "                    batch['src_features'], batch['dst_features'], batch['edge_features']\n",
    "                )\n",
    "                \n",
    "                val_preds.extend(outputs['link_prob'].cpu().numpy())\n",
    "                val_labels.extend(batch['label'].cpu().numpy())\n",
    "        \n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        val_ap = average_precision_score(val_labels, val_preds)\n",
    "        val_aucs.append(val_auc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s)\")\n",
    "        print(f\"  Train Loss: {avg_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "        print(f\"  Val AUC: {val_auc:.4f}, Val AP: {val_ap:.4f}\")\n",
    "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"  New best model saved (AUC: {best_val_auc:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_aucs': val_aucs,\n",
    "        'best_val_auc': best_val_auc\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_loader, verbose=True):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_src_embeddings = []\n",
    "    all_dst_embeddings = []\n",
    "    all_src_communities = []\n",
    "    all_dst_communities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                batch['src_idx'], batch['dst_idx'], batch['timestamp'],\n",
    "                batch['src_features'], batch['dst_features'], batch['edge_features']\n",
    "            )\n",
    "            \n",
    "            all_preds.extend(outputs['link_prob'].cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            all_src_embeddings.append(outputs['src_embeddings'].cpu().numpy())\n",
    "            all_dst_embeddings.append(outputs['dst_embeddings'].cpu().numpy())\n",
    "            all_src_communities.append(outputs['src_community'].cpu().numpy())\n",
    "            all_dst_communities.append(outputs['dst_community'].cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_auc = roc_auc_score(all_labels, all_preds)\n",
    "    test_ap = average_precision_score(all_labels, all_preds)\n",
    "    \n",
    "    # Threshold predictions\n",
    "    binary_preds = (np.array(all_preds) > 0.5).astype(int)\n",
    "    test_acc = accuracy_score(all_labels, binary_preds)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"AUC: {test_auc:.4f}\")\n",
    "        print(f\"AP: {test_ap:.4f}\")\n",
    "        print(f\"Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'auc': test_auc,\n",
    "        'ap': test_ap,\n",
    "        'accuracy': test_acc,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'src_embeddings': np.vstack(all_src_embeddings),\n",
    "        'dst_embeddings': np.vstack(all_dst_embeddings),\n",
    "        'src_communities': np.vstack(all_src_communities),\n",
    "        'dst_communities': np.vstack(all_dst_communities)\n",
    "    }\n",
    "\n",
    "# Train the model if everything is set up\n",
    "if dataset is not None and 'model' in locals():\n",
    "    print(\"Starting model training...\")\n",
    "    \n",
    "    training_results = train_model(\n",
    "        model, train_loader, val_loader, \n",
    "        num_epochs=30, lr=0.001, patience=8\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation AUC: {training_results['best_val_auc']:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_results = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(\"\\nTraining and evaluation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2d9a7",
   "metadata": {},
   "source": [
    "## 3. Enhanced Decay Factor TGN Model\n",
    "\n",
    "Implement the Temporal Graph Network with configurable decay-based temporal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDecayTGN(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Temporal Graph Network with decay-based temporal attention\n",
    "    Optimized for CUDA and includes community detection capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_nodes, node_feat_dim, edge_feat_dim, memory_dim=128, \n",
    "                 time_dim=32, embedding_dim=128, decay_factor=0.1, \n",
    "                 n_heads=4, dropout=0.1, device='cuda'):\n",
    "        super(EnhancedDecayTGN, self).__init__()\n",
    "        \n",
    "        self.num_nodes = num_nodes\n",
    "        self.node_feat_dim = node_feat_dim\n",
    "        self.edge_feat_dim = edge_feat_dim\n",
    "        self.memory_dim = memory_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decay_factor = decay_factor\n",
    "        self.n_heads = n_heads\n",
    "        self.device = device\n",
    "        \n",
    "        # Time encoder\n",
    "        self.time_encoder = nn.Sequential(\n",
    "            nn.Linear(1, time_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "        \n",
    "        # Node and edge embedding layers\n",
    "        self.node_embedding = nn.Linear(node_feat_dim, memory_dim)\n",
    "        self.edge_embedding = nn.Linear(edge_feat_dim, memory_dim)\n",
    "        \n",
    "        # Memory module\n",
    "        self.memory = nn.Parameter(torch.zeros(num_nodes, memory_dim), requires_grad=False)\n",
    "        self.last_update = nn.Parameter(torch.zeros(num_nodes), requires_grad=False)\n",
    "        self.memory_updater = nn.GRUCell(memory_dim, memory_dim)\n",
    "        \n",
    "        # Decay-based temporal attention\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            embed_dim=memory_dim,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Message function\n",
    "        self.message_function = nn.Sequential(\n",
    "            nn.Linear(memory_dim * 2 + time_dim, memory_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(memory_dim, memory_dim)\n",
    "        )\n",
    "        \n",
    "        # Embedding projection\n",
    "        self.embedding_projection = nn.Sequential(\n",
    "            nn.Linear(memory_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Link predictor\n",
    "        self.link_predictor = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Community detection head\n",
    "        self.community_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim // 2, 32)  # 32 potential communities\n",
    "        )\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def apply_decay(self, memory, current_time, last_update_time):\n",
    "        \"\"\"Apply exponential decay to memory based on time difference\"\"\"\n",
    "        time_diff = current_time - last_update_time\n",
    "        time_diff = torch.clamp(time_diff, min=0.0)\n",
    "        decay_weight = torch.exp(-self.decay_factor * time_diff).unsqueeze(-1)\n",
    "        return memory * decay_weight\n",
    "    \n",
    "    def update_memory(self, node_ids, timestamps, src_features, dst_features, edge_features):\n",
    "        \"\"\"Update node memory with temporal decay\"\"\"\n",
    "        # Get current memory\n",
    "        src_memory = self.memory[node_ids[:, 0]]\n",
    "        dst_memory = self.memory[node_ids[:, 1]]\n",
    "        \n",
    "        # Apply decay\n",
    "        src_memory = self.apply_decay(src_memory, timestamps, self.last_update[node_ids[:, 0]])\n",
    "        dst_memory = self.apply_decay(dst_memory, timestamps, self.last_update[node_ids[:, 1]])\n",
    "        \n",
    "        # Encode time\n",
    "        time_features = self.time_encoder(timestamps.unsqueeze(-1))\n",
    "        \n",
    "        # Create messages\n",
    "        src_messages = self.message_function(\n",
    "            torch.cat([src_features, dst_features, time_features], dim=-1)\n",
    "        )\n",
    "        dst_messages = self.message_function(\n",
    "            torch.cat([dst_features, src_features, time_features], dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Update memory using GRU\n",
    "        new_src_memory = self.memory_updater(src_messages, src_memory)\n",
    "        new_dst_memory = self.memory_updater(dst_messages, dst_memory)\n",
    "        \n",
    "        # Update memory in-place\n",
    "        self.memory[node_ids[:, 0]] = new_src_memory.detach()\n",
    "        self.memory[node_ids[:, 1]] = new_dst_memory.detach()\n",
    "        self.last_update[node_ids[:, 0]] = timestamps.detach()\n",
    "        self.last_update[node_ids[:, 1]] = timestamps.detach()\n",
    "        \n",
    "        return new_src_memory, new_dst_memory\n",
    "    \n",
    "    def get_node_embeddings(self, node_ids, timestamps, node_features):\n",
    "        \"\"\"Get node embeddings with temporal decay\"\"\"\n",
    "        # Get decayed memory\n",
    "        memory = self.memory[node_ids]\n",
    "        memory = self.apply_decay(memory, timestamps, self.last_update[node_ids])\n",
    "        \n",
    "        # Apply attention (self-attention for simplification)\n",
    "        memory_attended, _ = self.temporal_attention(memory.unsqueeze(1), memory.unsqueeze(1), memory.unsqueeze(1))\n",
    "        memory_attended = memory_attended.squeeze(1)\n",
    "        \n",
    "        # Project to final embedding\n",
    "        embeddings = self.embedding_projection(memory_attended)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, src_idx, dst_idx, timestamps, src_features, dst_features, edge_features):\n",
    "        \"\"\"Forward pass for link prediction\"\"\"\n",
    "        batch_size = src_idx.size(0)\n",
    "        node_ids = torch.stack([src_idx, dst_idx], dim=1)\n",
    "        \n",
    "        # Update memory\n",
    "        src_memory, dst_memory = self.update_memory(\n",
    "            node_ids, timestamps, src_features, dst_features, edge_features\n",
    "        )\n",
    "        \n",
    "        # Get node embeddings\n",
    "        src_embeddings = self.get_node_embeddings(src_idx, timestamps, src_features)\n",
    "        dst_embeddings = self.get_node_embeddings(dst_idx, timestamps, dst_features)\n",
    "        \n",
    "        # Link prediction\n",
    "        link_features = torch.cat([src_embeddings, dst_embeddings], dim=-1)\n",
    "        link_prob = self.link_predictor(link_features)\n",
    "        \n",
    "        # Community detection (optional)\n",
    "        src_community = self.community_head(src_embeddings)\n",
    "        dst_community = self.community_head(dst_embeddings)\n",
    "        \n",
    "        return {\n",
    "            'link_prob': link_prob.squeeze(-1),\n",
    "            'src_embeddings': src_embeddings,\n",
    "            'dst_embeddings': dst_embeddings,\n",
    "            'src_community': src_community,\n",
    "            'dst_community': dst_community\n",
    "        }\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Reset memory state\"\"\"\n",
    "        self.memory.data.zero_()\n",
    "        self.last_update.data.zero_()\n",
    "    \n",
    "    def get_all_embeddings(self, timestamps=None):\n",
    "        \"\"\"Get embeddings for all nodes\"\"\"\n",
    "        if timestamps is None:\n",
    "            timestamps = torch.zeros(self.num_nodes, device=self.device)\n",
    "        \n",
    "        node_ids = torch.arange(self.num_nodes, device=self.device)\n",
    "        dummy_features = torch.zeros(self.num_nodes, self.node_feat_dim, device=self.device)\n",
    "        \n",
    "        return self.get_node_embeddings(node_ids, timestamps, dummy_features)\n",
    "\n",
    "# Initialize the model if dataset is available\n",
    "if dataset is not None:\n",
    "    model = EnhancedDecayTGN(\n",
    "        num_nodes=dataset['num_nodes'],\n",
    "        node_feat_dim=dataset['node_feat_dim'],\n",
    "        edge_feat_dim=dataset['edge_feat_dim'],\n",
    "        memory_dim=128,\n",
    "        time_dim=32,\n",
    "        embedding_dim=128,\n",
    "        decay_factor=0.1,  # Configurable decay factor\n",
    "        n_heads=4,\n",
    "        dropout=0.1,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c51c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Comprehensive Link Prediction and Community Detection with Decay Factor TGN\n",
    "\n",
    "This notebook implements a complete system for link prediction and community detection using Temporal Graph Networks (TGN) with decay-based temporal attention. The implementation includes:\n",
    "\n",
    "1. **Decay Factor TGN Model**: Enhanced temporal attention with configurable decay factors\n",
    "2. **Link Prediction**: Predicting future connections in temporal graphs\n",
    "3. **Community Detection**: Identifying and tracking community evolution over time\n",
    "4. **Comprehensive Visualizations**: Graph structures, temporal evolution, and community dynamics\n",
    "5. **CUDA Optimization**: GPU acceleration for all computations\n",
    "\n",
    "Dataset: Reddit Hyperlinks - representing connections between subreddit communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4c186",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import community.community_louvain as community_louvain\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append('../src')\n",
    "sys.path.append('src')\n",
    "\n",
    "# Import custom modules\n",
    "from decay_tgn import DecayTemporalGraphNetwork, create_decay_tgn_model, analyze_decay_effects\n",
    "from enhanced_tgn import TemporalGraphNetwork\n",
    "\n",
    "# Set up device (prioritize CUDA)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Configure matplotlib and seaborn for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a256035",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "Load and preprocess the Reddit Hyperlinks dataset for temporal graph analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a73c2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_reddit_dataset(data_path='data/', sample_size=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess the Reddit dataset for TGN training\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to data directory\n",
    "        sample_size: If specified, sample this many edges for faster processing\n",
    "        verbose: Print detailed information\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing processed data splits and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try multiple file formats\n",
    "        reddit_files = [\n",
    "            'reddit_TGAT.csv',\n",
    "            'soc-redditHyperlinks-title.tsv',\n",
    "            'soc-redditHyperlinks-body.tsv'\n",
    "        ]\n",
    "        \n",
    "        df = None\n",
    "        for filename in reddit_files:\n",
    "            filepath = os.path.join(data_path, filename)\n",
    "            if os.path.exists(filepath):\n",
    "                if verbose:\n",
    "                    print(f\"Loading {filename}...\")\n",
    "                \n",
    "                if filename.endswith('.csv'):\n",
    "                    df = pd.read_csv(filepath)\n",
    "                else:\n",
    "                    df = pd.read_csv(filepath, sep='\\t')\n",
    "                break\n",
    "        \n",
    "        if df is None:\n",
    "            raise FileNotFoundError(\"No Reddit dataset found in data directory\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Loaded dataset with {len(df)} edges\")\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Standardize column names\n",
    "        column_mapping = {\n",
    "            'SOURCE_SUBREDDIT': 'src',\n",
    "            'TARGET_SUBREDDIT': 'dst',\n",
    "            'TIMESTAMP': 'timestamp',\n",
    "            'LINK_SENTIMENT': 'sentiment',\n",
    "            'PROPERTIES': 'properties'\n",
    "        }\n",
    "        \n",
    "        # Rename columns if they exist\n",
    "        for old_name, new_name in column_mapping.items():\n",
    "            if old_name in df.columns:\n",
    "                df = df.rename(columns={old_name: new_name})\n",
    "        \n",
    "        # If standard names not found, use first two columns as src/dst\n",
    "        if 'src' not in df.columns:\n",
    "            df = df.rename(columns={df.columns[0]: 'src', df.columns[1]: 'dst'})\n",
    "        \n",
    "        # Sample data if requested for faster processing\n",
    "        if sample_size and len(df) > sample_size:\n",
    "            df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "            if verbose:\n",
    "                print(f\"Sampled {sample_size} edges for processing\")\n",
    "        \n",
    "        # Process timestamps\n",
    "        if 'timestamp' in df.columns:\n",
    "            # Convert to datetime and then to numerical\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "            df['timestamp_numeric'] = df['timestamp'].astype(int) / 10**9\n",
    "        else:\n",
    "            # Create artificial timestamps\n",
    "            df['timestamp_numeric'] = np.arange(len(df), dtype=float)\n",
    "            if verbose:\n",
    "                print(\"Created artificial timestamps\")\n",
    "        \n",
    "        # Remove rows with invalid timestamps\n",
    "        df = df.dropna(subset=['timestamp_numeric'])\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        df = df.sort_values('timestamp_numeric').reset_index(drop=True)\n",
    "        \n",
    "        # Create node mapping\n",
    "        all_nodes = pd.concat([df['src'], df['dst']]).unique()\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "        idx_to_node = {idx: node for node, idx in node_to_idx.items()}\n",
    "        \n",
    "        # Map to indices\n",
    "        df['src_idx'] = df['src'].map(node_to_idx)\n",
    "        df['dst_idx'] = df['dst'].map(node_to_idx)\n",
    "        \n",
    "        num_nodes = len(all_nodes)\n",
    "        \n",
    "        # Extract edge features\n",
    "        edge_feat_dim = 50\n",
    "        if 'sentiment' in df.columns:\n",
    "            # Use sentiment as one feature and pad with random features\n",
    "            sentiment_values = pd.to_numeric(df['sentiment'], errors='coerce').fillna(0)\n",
    "            edge_features = np.random.randn(len(df), edge_feat_dim)\n",
    "            edge_features[:, 0] = sentiment_values  # First feature is sentiment\n",
    "        else:\n",
    "            # Random edge features\n",
    "            edge_features = np.random.randn(len(df), edge_feat_dim)\n",
    "        \n",
    "        # Create node features (random initialization)\n",
    "        node_feat_dim = 100\n",
    "        node_features = np.random.randn(num_nodes, node_feat_dim)\n",
    "        \n",
    "        # Split data temporally (70% train, 15% val, 15% test)\n",
    "        train_ratio, val_ratio = 0.7, 0.15\n",
    "        train_end = int(len(df) * train_ratio)\n",
    "        val_end = int(len(df) * (train_ratio + val_ratio))\n",
    "        \n",
    "        train_df = df.iloc[:train_end].copy()\n",
    "        val_df = df.iloc[train_end:val_end].copy()\n",
    "        test_df = df.iloc[val_end:].copy()\n",
    "        \n",
    "        # Create edge features for each split\n",
    "        train_edge_features = edge_features[:train_end]\n",
    "        val_edge_features = edge_features[train_end:val_end]\n",
    "        test_edge_features = edge_features[val_end:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nDataset Statistics:\")\n",
    "            print(f\"Total nodes: {num_nodes:,}\")\n",
    "            print(f\"Total edges: {len(df):,}\")\n",
    "            print(f\"Train edges: {len(train_df):,}\")\n",
    "            print(f\"Validation edges: {len(val_df):,}\")\n",
    "            print(f\"Test edges: {len(test_df):,}\")\n",
    "            print(f\"Node feature dim: {node_feat_dim}\")\n",
    "            print(f\"Edge feature dim: {edge_feat_dim}\")\n",
    "            print(f\"Time range: {df['timestamp_numeric'].min():.0f} - {df['timestamp_numeric'].max():.0f}\")\n",
    "        \n",
    "        return {\n",
    "            'train_df': train_df,\n",
    "            'val_df': val_df,\n",
    "            'test_df': test_df,\n",
    "            'train_edge_features': train_edge_features,\n",
    "            'val_edge_features': val_edge_features,\n",
    "            'test_edge_features': test_edge_features,\n",
    "            'node_features': node_features,\n",
    "            'num_nodes': num_nodes,\n",
    "            'node_feat_dim': node_feat_dim,\n",
    "            'edge_feat_dim': edge_feat_dim,\n",
    "            'node_to_idx': node_to_idx,\n",
    "            'idx_to_node': idx_to_node,\n",
    "            'full_df': df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading Reddit dataset...\")\n",
    "dataset = load_reddit_dataset(sample_size=50000)  # Sample for faster processing\n",
    "\n",
    "if dataset is None:\n",
    "    print(\"Failed to load dataset. Please check data files.\")\n",
    "else:\n",
    "    print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f5d7f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 2. Temporal Graph Dataset Class\n",
    "\n",
    "Create a PyTorch Dataset class for efficient batch processing of temporal graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5941d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGraphDataset(Dataset):\n",
    "    \"\"\"Dataset class for temporal graph data\"\"\"\n",
    "    \n",
    "    def __init__(self, df, edge_features, node_features, negative_sampling_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with columns ['src_idx', 'dst_idx', 'timestamp_numeric']\n",
    "            edge_features: Edge features array\n",
    "            node_features: Node features array\n",
    "            negative_sampling_ratio: Ratio of negative to positive samples\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.edge_features = edge_features\n",
    "        self.node_features = node_features\n",
    "        self.negative_sampling_ratio = negative_sampling_ratio\n",
    "        self.num_nodes = len(node_features)\n",
    "        \n",
    "        # Create positive samples\n",
    "        self.positive_samples = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            self.positive_samples.append({\n",
    "                'src': row['src_idx'],\n",
    "                'dst': row['dst_idx'],\n",
    "                'timestamp': row['timestamp_numeric'],\n",
    "                'edge_features': self.edge_features[idx],\n",
    "                'label': 1\n",
    "            })\n",
    "        \n",
    "        # Generate negative samples\n",
    "        self.negative_samples = self._generate_negative_samples()\n",
    "        \n",
    "        # Combine positive and negative samples\n",
    "        self.samples = self.positive_samples + self.negative_samples\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.positive_samples)} positive and {len(self.negative_samples)} negative samples\")\n",
    "    \n",
    "    def _generate_negative_samples(self):\n",
    "        \"\"\"Generate negative samples by random node sampling\"\"\"\n",
    "        negative_samples = []\n",
    "        num_negatives = int(len(self.positive_samples) * self.negative_sampling_ratio)\n",
    "        \n",
    "        # Create set of existing edges for each timestamp\n",
    "        existing_edges = set()\n",
    "        for _, row in self.df.iterrows():\n",
    "            existing_edges.add((row['src_idx'], row['dst_idx']))\n",
    "        \n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        for i in range(num_negatives):\n",
    "            # Sample a random positive sample to get timestamp\n",
    "            pos_idx = np.random.randint(0, len(self.positive_samples))\n",
    "            timestamp = self.positive_samples[pos_idx]['timestamp']\n",
    "            \n",
    "            # Sample random nodes that don't form an existing edge\n",
    "            max_attempts = 100\n",
    "            for _ in range(max_attempts):\n",
    "                src = np.random.randint(0, self.num_nodes)\n",
    "                dst = np.random.randint(0, self.num_nodes)\n",
    "                \n",
    "                if src != dst and (src, dst) not in existing_edges:\n",
    "                    negative_samples.append({\n",
    "                        'src': src,\n",
    "                        'dst': dst,\n",
    "                        'timestamp': timestamp,\n",
    "                        'edge_features': np.random.randn(self.edge_features.shape[1]),\n",
    "                        'label': 0\n",
    "                    })\n",
    "                    break\n",
    "        \n",
    "        return negative_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        return {\n",
    "            'src_idx': torch.tensor(sample['src'], dtype=torch.long),\n",
    "            'dst_idx': torch.tensor(sample['dst'], dtype=torch.long),\n",
    "            'timestamp': torch.tensor(sample['timestamp'], dtype=torch.float),\n",
    "            'edge_features': torch.tensor(sample['edge_features'], dtype=torch.float),\n",
    "            'src_features': torch.tensor(self.node_features[sample['src']], dtype=torch.float),\n",
    "            'dst_features': torch.tensor(self.node_features[sample['dst']], dtype=torch.float),\n",
    "            'label': torch.tensor(sample['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create datasets if data loading was successful\n",
    "if dataset is not None:\n",
    "    train_dataset = TemporalGraphDataset(\n",
    "        dataset['train_df'], \n",
    "        dataset['train_edge_features'], \n",
    "        dataset['node_features'],\n",
    "        negative_sampling_ratio=1.0\n",
    "    )\n",
    "    \n",
    "    val_dataset = TemporalGraphDataset(\n",
    "        dataset['val_df'], \n",
    "        dataset['val_edge_features'], \n",
    "        dataset['node_features'],\n",
    "        negative_sampling_ratio=1.0\n",
    "    )\n",
    "    \n",
    "    test_dataset = TemporalGraphDataset(\n",
    "        dataset['test_df'], \n",
    "        dataset['test_edge_features'], \n",
    "        dataset['node_features'],\n",
    "        negative_sampling_ratio=1.0\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 512 if torch.cuda.is_available() else 128\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"\\nData loaders created with batch size: {batch_size}\")\n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef129dc2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
